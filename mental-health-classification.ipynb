{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11251022,"sourceType":"datasetVersion","datasetId":7030651}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T10:56:21.266642Z","iopub.execute_input":"2025-04-02T10:56:21.267029Z","iopub.status.idle":"2025-04-02T10:56:21.700416Z","shell.execute_reply.started":"2025-04-02T10:56:21.267000Z","shell.execute_reply":"2025-04-02T10:56:21.699263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Load the dataset\nfile_path = \"/kaggle/input/mental-health-dataset/Mental Health Dataset.csv\"\ndata = pd.read_csv(file_path)\n\n# Drop rows with missing values\ndata = data.dropna()\n\n# Encode categorical variables\nlabel_encoders = {}\nfor column in data.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])\n    label_encoders[column] = le\n\n# Separate features and target variable\nX = data.drop(columns=[\"treatment\"])\ny = data[\"treatment\"]\n\n# Feature Selection\nselector = SelectKBest(score_func=f_classif, k=10)  # Select top 10 features\nX_selected = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support()]\nprint(f\"Selected Features: {selected_features.tolist()}\")\n\n# Standardize the selected features\nscaler = StandardScaler()\nX_selected = scaler.fit_transform(X_selected)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n\n# Hyperparameter Tuning for Random Forest\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nbest_rf_model = grid_search.best_estimator_\nprint(f\"Best Random Forest Parameters: {grid_search.best_params_}\")\n\n# Train Gradient Boosting with optimized parameters\ngb_model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)\ngb_model.fit(X_train, y_train)\n\n# Model Evaluation\ndef evaluate_model(model, name):\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"\\n{name} Accuracy: {accuracy:.4f}\")\n    print(classification_report(y_test, y_pred))\n    \n    conf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"Confusion Matrix - {name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.show()\n\nevaluate_model(best_rf_model, \"Random Forest\")\nevaluate_model(gb_model, \"Gradient Boosting\")\n\n# Make predictions\nrf_predictions = best_rf_model.predict(X_test)\ngb_predictions = gb_model.predict(X_test)\n\n# Save predictions to CSV\npredictions_df = pd.DataFrame({\n    \"Actual\": y_test.values,\n    \"Random_Forest_Predictions\": rf_predictions,\n    \"Gradient_Boosting_Predictions\": gb_predictions\n})\n\npredictions_df.to_csv(\"predictions.csv\", index=False)\nprint(\"Predictions saved to predictions.csv successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T10:57:16.411384Z","iopub.execute_input":"2025-04-02T10:57:16.411995Z","iopub.status.idle":"2025-04-02T11:17:50.289249Z","shell.execute_reply.started":"2025-04-02T10:57:16.411955Z","shell.execute_reply":"2025-04-02T11:17:50.288030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/predictions.csv\")\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:23:31.549480Z","iopub.execute_input":"2025-04-02T11:23:31.549953Z","iopub.status.idle":"2025-04-02T11:23:31.574523Z","shell.execute_reply.started":"2025-04-02T11:23:31.549922Z","shell.execute_reply":"2025-04-02T11:23:31.573127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load dataset\ndata = pd.read_csv(\"/kaggle/input/mental-health-dataset/Mental Health Dataset.csv\")\n\n# Drop Timestamp column as it isn't useful\ndata = data.drop(columns=['Timestamp'])\n\n# Text cleaning function\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n    return text\n\n# Feature extraction function\ndef extract_features(text):\n    words = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [w for w in words if w not in stop_words]\n    \n    return {\n        'token_count': len(words),\n        'sentence_length': len(text.split()),\n        'stopword_count': len(words) - len(filtered_words),\n        'negative_words': sum(1 for w in words if w in ['bad', 'sad', 'stress', 'anxiety'])\n    }\n\n# Apply text preprocessing\ndata['Cleaned_Text'] = data['care_options'].apply(clean_text)\nfeatures = data['Cleaned_Text'].apply(extract_features).apply(pd.Series)\n\n# TF-IDF feature extraction\ntfidf = TfidfVectorizer(max_features=500)\ntfidf_features = tfidf.fit_transform(data['Cleaned_Text']).toarray()\ntfidf_df = pd.DataFrame(tfidf_features, columns=tfidf.get_feature_names_out())\n\n# Handling the problematic 'Days_Indoors' column\n# Convert to a categorical feature using one-hot encoding\ndata = pd.get_dummies(data, columns=['Days_Indoors'], prefix='days')\n\n# Encode categorical features\ncategorical_cols = ['Gender', 'Country', 'Occupation', 'self_employed', \n                   'family_history', 'treatment', 'mental_health_interview', \n                   'Growing_Stress', 'Changes_Habits', 'Mental_Health_History',\n                   'Coping_Struggles', 'Work_Interest', 'Social_Weakness']\n\n# Label Encoding for categorical columns\nlabel_enc = LabelEncoder()\nfor col in categorical_cols:\n    if col in data.columns:  # Check if column exists\n        if data[col].dtype == 'object' or data[col].dtype == 'bool':  # Only encode object or boolean types\n            data[col] = label_enc.fit_transform(data[col].astype(str))\n\n# Encode Mood_Swings if not already numeric\nif data['Mood_Swings'].dtype == 'object':\n    data['Mood_Swings'] = label_enc.fit_transform(data['Mood_Swings'])\n\n# One-Hot Encoding for categorical columns with many unique values\n# Country and Occupation have already been handled with get_dummies\n\n# Drop processed text column and concat with feature dataframes\ndata = pd.concat([data.drop(columns=['Cleaned_Text', 'care_options']), features, tfidf_df], axis=1)\n\n# Print dtypes to verify all columns are numeric before modeling\nprint(data.dtypes)\n\n# Define target and features\nX = data.drop(columns=['Mood_Swings'])  # Target is Mood_Swings\ny = data['Mood_Swings']\n\n# Train/Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Model selection\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier()\n}\n\nbest_model = None\nbest_score = 0\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test, preds)\n    print(f\"{name} Accuracy: {score:.4f}\")\n    if score > best_score:\n        best_score = score\n        best_model = model\n\n# Fine-tuning using Grid Search\nparam_grid = {\n    'n_estimators': [50, 100, 200], \n    'max_depth': [10, 20, None]\n} if isinstance(best_model, RandomForestClassifier) else {\n    'C': [0.1, 1, 10]\n} if isinstance(best_model, LogisticRegression) else {\n    'max_depth': [10, 20, None]\n}\n\ntuned_model = GridSearchCV(best_model, param_grid, cv=5, scoring='accuracy')\ntuned_model.fit(X_train, y_train)\ny_pred = tuned_model.predict(X_test)\n\n# Display results\nprint(\"Best Model Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Save predictions\nsubmission = pd.DataFrame({'ID': X_test.index, 'Mood_Swings': y_pred})\nsubmission.to_csv('predictionss.csv', index=False)\nprint(\"Predictions saved to 'predictionss.csv'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:34:32.768147Z","iopub.execute_input":"2025-04-02T12:34:32.768584Z","iopub.status.idle":"2025-04-02T12:52:31.632181Z","shell.execute_reply.started":"2025-04-02T12:34:32.768533Z","shell.execute_reply":"2025-04-02T12:52:31.630979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/predictionss.csv\")\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T12:54:02.337938Z","iopub.execute_input":"2025-04-02T12:54:02.338278Z","iopub.status.idle":"2025-04-02T12:54:02.358495Z","shell.execute_reply.started":"2025-04-02T12:54:02.338252Z","shell.execute_reply":"2025-04-02T12:54:02.357364Z"}},"outputs":[],"execution_count":null}]}